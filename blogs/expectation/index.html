<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><title>Virie's weblog</title><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=icon type=image/svg+xml href=../../resources/Virie_symbol_2023.svg><link rel=stylesheet href=../../styles/effect.css><link rel=stylesheet href=../../styles/banner.css><link rel=stylesheet href=../../styles/theme.css><link rel=stylesheet href=../styles/layout.css><link rel=stylesheet href=../styles/theme.css><link rel=stylesheet href=./styles.css></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-45PD6BDYNZ"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-45PD6BDYNZ');
</script><body class=neutral-2><div class="banner active secondary"><div class=title><h1 id=virie-header class=relief></h1></div><div class="menu relief"><a href=../../index.html>home</a><a href=../../weblogs.html>weblogs</a><a href=../../profile.html>profile</a><a href=../../persona.html>persona</a></div></div><div class="navigate tertiary silver-lining-bottom"><a href=../polynomial/index.html>prev</a><a href=../../weblogs.html class=date>April 5th, 2023</a><a href=../progress/index.html>next</a></div><div class="blog primary"><div class=content><h1>&#8747 The reign of expectation terror</h1><hr><div class=kernel><h2> Kernel: </h2><ul><li> In a certain expectation game, there's a finite convergence strategy. </li></ul></div><p class=first> Humans are tenacious, persistent, and hardly know when to quit, whether it's about when to stop playing games and go to sleep, or when to sell holding stocks to cut loss. In computer science vernacular, it's called a "stopping problem". </p><p> I've read a great blog by Samir Khan<span class=cite>https://patternsofideas.github.io/posts/expectations/</span>. He argued that in some scenarios, when it comes to choose the optimal stopping, not even a great tool such as the expectation of payoff helps. </p><p class=first>Imagine a game of sequential bets where at each step we could decide to stop and take all the credit we've collected so far or we could take a chance to double it, only if the chance fails then we lose it all.</p><div class="state-diagram flow-horizontal"> { "node": "r", "transition": { "p": { "node": "2r", "transition": { "p": { "node": "4r" }, "1-p": { "node": "0" } } }, "1-p": { "node": "0" } } } </div><div class=mathjax> $$ \begin{align*} E[\text{next payoff} | \text{bet}] &= p2r + (1-p)0 \\ E[\text{next payoff} | \text{stop}] &= 0 \\ \end{align*} $$ </div><p> We could try to do some math to decide whether should we continue at each step. But if the chance \(p > 0.5\) You will see that the expected payoff favors we take the bet. Yet if there's anything we learn from probability, we know that eventually we would fail if we don't stop. That we will surely get back empty-handed, or continue betting until we do. So expectation fails us! ... or not? </p><p> The reason why we use expectation is to marginalize out all uncontrollable factors, and that reason is pretty solid. If we follow the expectation yet the outcome does not making sense. Perhaps your reward function does not truly reflect what you feel? </p><p> Let's us consider the scenario where we don't actually play the game. We only input a number, say k , that tells the game how many times we would take the bets, the game itself will then instantly resolve the game and display the outcome to you. When treating the game this way, I feel more comfortable taking a larger \(k\) from the fact that now I've gotten rid of in-game cognitive biases. </p><div class=mathjax> $$ \begin{align*} E[\text{payoff} | \text{stop at}\,k] &= r(2p)^k \end{align*} $$ </div><p> Humans have so many cognitive biases. One in active here is called loss aversion. When the stakes are high enough, gaining more might not be more important than preventing loss. Thus we incline to stop to collect what we already have than going back to nothing. This seems like the bias works against us to define what's the best interest. </p><p> Still I would not choose \(k = \infty\), despite the fact that it gives the best expected payoff; i.e., there is a slim probability that I would get a handsome reward. Am I wrong? Or are there some other cognitive biases in work here? If not, then may be the expectation fails me? </p><p class=first> While I enjoy Khan's premise and statements, I have to disagree with his conclusion. It's never been about expectation alone. It's also about policies, and we have to use the expectation to derive a sensible policy. One that does not involve betting to the infinity to get the reward. </p><p> Suppose that at each step, I'll toss a biased coin that goes head with probability \(q\). If it comes out head, I'll continue playing; otherwise I'll stop and the current reward. What is the expected payoff of this policy? </p><div class="state-diagram flow-horizontal"> { "node": "r", "transition": { "q": { "node": "r", "transition": { "p": { "node": "2r", "transition": { "q": { "node": "2r", "transition": { "p": { "node": "4r" }, "1-p": { "node": "0" } } }, "1-q": { "node": "2r" } } }, "1-p": { "node": "0" } } }, "1-q": { "node": "r" } } } </div><div class=mathjax> $$ \begin{align*} E[\text{payoff} | q] &= r\frac{1-q}{1-q2p} \end{align*} $$ </div><p> The expected payoff of this random policy goes to infinity when \(q\) approaches \(\frac{1}{2p}\). Meaning sometimes I would secure the reward, sometimes I will continue, sometimes I will get nothing, but in overall the expected payoff is infinity just like the case where I never stop in the first policy. Now we don't leave our fate at the hand of infinitesimal god, but rather the marvel of mathematics. </p><p> Now which policy is faster? Faster means I could expectedly get the same reward while enjoy a shorter game. We could analytically show that the expected stopping point of the random policy is always sooner than the first policy at the same expected payoff. </p> Given an expected payoff of \(xr > 0\), I could derive the parameters of both policies: <div class=mathjax> $$ \begin{align*} (2p)^k = x &\rightarrow k = \frac{\log x}{\log 2p} \\ \frac{1-q}{1-q2p} = x &\rightarrow q = \frac{1 - x}{1 - x2p} \end{align*} $$ </div> The expected number of steps taken to stop under these parameters: <div class=mathjax> $$ \begin{align*} E[\text{steps} | k] &= \frac{p^k - 1}{p - 1} \\ E[\text{steps} | q] &= \frac{q}{1-qp} \end{align*} $$ </div> They both are of equal speed when \(p = 0.5\): <div class=mathjax> $$ \begin{align*} k = \frac{\log x}{0} = \infty &\rightarrow \frac{p^k - 1}{p - 1} = 2 \\ q = \frac{1-x}{1-x} = 1 &\rightarrow \frac{q}{1-qp} = 2 \end{align*} $$ </div> The random policy is faster when \(\frac{1}{2} < p < 1, x \rightarrow \infty \): <div class=mathjax> $$ \begin{align*} \frac{p^k - 1}{p - 1} - \frac{q}{1 - qp} &= \frac{(1-p^k)(p(x+1)-1) - (1-p)(x-1)}{(1-p)(p(x+1) - 1)} \\ &= \frac{(p(x+1)-1) - (1-p)(x-1)}{(1-p)(p(x+1) - 1)} \quad \text{since} \quad k \rightarrow \infty \\ &= \frac{(2p - 1)x}{(1-p)(p(x+1) - 1)} > 0 \end{align*} $$ </div><p class=first> So expectation has yet to fail. Because we're not only looking at actions but on the scale of policies when considering the expectation. An intelligence agent is one that chooses policies that is most efficient for the best expected payoff. </p><hr><ul class=bibliography></ul></div></div><script src=../../libs/svg.min.js></script><script src=../scripts/bibliography.js></script><script src=./graphic.js></script><script src=../../scripts/viewer.js></script><script src=../../scripts/active.js></script><script src=../scripts/weblog.js></script></body></html>